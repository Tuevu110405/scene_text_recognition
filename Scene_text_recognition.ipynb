{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrQn9e/SamYjgB+SSQbrhQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuevu110405/scene_text_recognition/blob/feature%2Ftraining/Scene_text_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTYsx25Wps-G",
        "outputId": "0e2ac88d-f5b3-45ad-e69a-38feb5283aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.57-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.57-py3-none-any.whl (905 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m905.3/905.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.57 ultralytics-thop-2.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqM9tdzvox2c",
        "outputId": "ba21c0af-adc2-430c-b1cb-f785426b2b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.57 üöÄ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 32.7/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "import ultralytics\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "ultralytics.checks()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_from_xml(root_dir):\n",
        "    xml_path = os.path.join(root_dir, 'data.xml')\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    img_paths = []\n",
        "    img_sizes = []\n",
        "    img_labels = []\n",
        "    bboxes = []\n",
        "\n",
        "    for img in root:\n",
        "        bbs_of_img = []\n",
        "        labels_of_img = []\n",
        "\n",
        "        for bbs in img.findall('taggedRectangles'):\n",
        "            for bb in bbs:\n",
        "                if not bb[0].text.isalnum():\n",
        "                    continue\n",
        "\n",
        "                if ' ' in bb[0].text.lower() or ' ' in bb[0].text.lower():\n",
        "                    continue\n",
        "\n",
        "                bbs_of_img.append(\n",
        "                    [\n",
        "                        float(bb.attrib['x']),\n",
        "                        float(bb.attrib['y']),\n",
        "                        float(bb.attrib['width']),\n",
        "                        float(bb.attrib['height'])\n",
        "                    ]\n",
        "                )\n",
        "                labels_of_img.append(bb[0].text.lower())\n",
        "\n",
        "        img_path = os.path.join(root_dir, img[0].text)\n",
        "        img_paths.append(img_path)\n",
        "        img_sizes.append((int(img.attrib['x']), int(img.attrib['y'])))\n",
        "        bboxes.append(bbs_of_img)\n",
        "        img_labels.append(labels_of_img)\n",
        "\n",
        "    return img_paths, img_sizes, img_labels, bboxes\n",
        "\n",
        "dataset_dir = 'SceneTrainTrain'\n",
        "img_paths, img_sizes, img_labels, bboxes = extract_data_from_xml(dataset_dir)"
      ],
      "metadata": {
        "id": "Qm4t-RKqppy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert into Yolo format\n",
        "def convert_to_yolo_format(image_paths, image_sizes, bounding_boxes):\n",
        "    yolo_data = []\n",
        "\n",
        "    for image_path, image_size, bboxes in zip(image_paths, image_sizes, bounding_boxes):\n",
        "        image_width, image_height = image_size\n",
        "\n",
        "        yolo_labels = []\n",
        "\n",
        "        for bbox in bboxes:\n",
        "            x, y, w, h = bbox\n",
        "            x_center = (x + w / 2) / image_width\n",
        "            y_center = (y + h / 2) / image_height\n",
        "            width = w / image_width\n",
        "            height = h / image_height\n",
        "\n",
        "            class_id = 0\n",
        "\n",
        "            #Convert yolo format\n",
        "            yolo_label = f\"{class_id} {x_center} {y_center} {width} {height}\"\n",
        "            yolo_labels.append(yolo_label)\n",
        "\n",
        "        yolo_data.append((image_path, yolo_labels))\n",
        "\n",
        "    return yolo_data\n",
        "\n",
        "#Define class labels\n",
        "class_labels = ['text']\n",
        "\n",
        "#Convert data into Yolo format\n",
        "class_labels = ['text']\n",
        "\n",
        "#Convert data into yolo format\n",
        "yolo_data = convert_to_yolo_format(imgae_paths, image_sizes, bounding_boxes)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f9gCcl0tvLlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(data, scr_img_dir, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    #make images and labels folder\n",
        "    os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "    for image_path, yolo_labels in data:\n",
        "        shutil.copy(\n",
        "            os.path.join(scr_img_dir,image_path),os.path.join(\n",
        "                save_dir, \"images\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        image_name = os.path.basename(image_path)\n",
        "        image_name = os.path.splitext(image_name)[0]\n",
        "\n",
        "        with open(os.path.join(save_dir, 'labels', image_name + '.txt'), 'w') as f:\n",
        "            for label in yolo_labels:\n",
        "                f.write(f\"{label}\\n\")\n",
        "\n",
        "seed = 0\n",
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "train_data, test_data = train_test_split(yolo_data, test_size=test_size, random_state=seed, shuffle=is_shuffle)\n",
        "\n",
        "test_data, val_data = train_test_split(test_data, test_size=test_size, random_state=seed, shuffle=is_shuffle)\n",
        "\n",
        "save_yolo_data_dir = \"datasets/yolo_data\"\n",
        "os.makedirs(save_yolo_data_dir, exist_ok = True)\n",
        "save_train_dir = os.path.join(save_yolo_data_dir, 'train')\n",
        "save_val_dir = os.path.join(save_yolo_data_dir, 'val')\n",
        "save_test_dir = os.path.join(save_yolo_data_dir, 'test')\n",
        "\n",
        "save_data(train_data, dataset_dir, save_train_dir)\n",
        "save_data(val_data, dataset_dir, save_val_dir)\n",
        "save_data(test_data, dataset_dir, save_test_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "YZL0HKXnQlJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_yaml = {\n",
        "    \"path\": \"./datasets/yolo_data\",\n",
        "    \"train\": \"train/images\",\n",
        "    \"val\": \"val/images\",\n",
        "    \"test\": \"test/images\",\n",
        "    \"nc\" : 1,\n",
        "    \"names\" : class_labels\n",
        "}\n",
        "\n",
        "yolo_yaml_path = os.path.join(save_yolo_data_dir, 'data.yaml')\n",
        "with open(yolo_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_yaml, f, default_flow_style=False)"
      ],
      "metadata": {
        "id": "vc3B5vxrbYCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11m.pt\")\n",
        "\n",
        "results = model.train(\n",
        "    data = yolo_yaml_path,\n",
        "    epochs = 100\n",
        "    imgsz = 640\n",
        "    cache = True\n",
        "    patience= 20\n",
        "    plots = True\n",
        ")"
      ],
      "metadata": {
        "id": "K7UVI75Scuh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'runs/detect/train/weights/best.pt'\n",
        "model = YOLO(model_path)\n",
        "\n",
        "metrics = model.val()"
      ],
      "metadata": {
        "id": "uBSg_hu8hcDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "3GfKXTWyhqzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_from_xml(root_dir):\n",
        "    xml_path = os.path.join(root_dir, 'data.xml')\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    img_paths = []\n",
        "    img_size = []\n",
        "    img_labels = []\n",
        "    bboxes = []\n",
        "\n",
        "    for img in root:\n",
        "        bbs_of_img = []\n",
        "        labels_of_img = []\n",
        "\n",
        "        for bbs in img.findall('taggedRectangles'):\n",
        "            for bb in bbs:\n",
        "\n",
        "                if not bb[0].text.isalnum():\n",
        "                    continue\n",
        "\n",
        "                if \"e\" in bb[0].text.lower() or \"n\" in bb[0].text.lower():\n",
        "                    continue\n",
        "\n",
        "                bbs_of_img.append(\n",
        "                    [\n",
        "                        float(bb.attrib['x']),\n",
        "                        float(bb.attrib['y']),\n",
        "                        float(bb.attrib['width']),\n",
        "                        float(bb.attrib['height'])\n",
        "                    ]\n",
        "                )\n",
        "                labels_of_img.append(bb[0].text.lower())\n",
        "\n",
        "        img_path = os.path.join(root_dir, img[0].text)\n",
        "        img_paths.append(img_path)\n",
        "        img_size.append((int(img.attrib['x']), int(img.attrib['y'])))\n",
        "        bboxes.append(bbs_of_img)\n",
        "        img_labels.append(labels_of_img)\n",
        "\n",
        "    return img_paths, img_size, img_labels, bboxes\n",
        "\n",
        "dataset_dir = 'datasets/SceneTrainTrain'\n",
        "img_paths, img_size, img_labels, bboxes = extract_data_from_xml(dataset_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "twN1fwS5lsbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_bounding_boxes(img_paths, img_labels, bboxes):\n",
        "    os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "    count = 0\n",
        "    labels = []\n",
        "\n",
        "    for img_path, img_label, bbs in zip(img_paths, img_labels, bboxes):\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        for label, bb in zip(img_label, bbs):\n",
        "            cropped_img = img.crop((bb[0], bb[1], bb[0] + bb[2], bb[1] + bb[3]))\n",
        "\n",
        "            if np.mean(cropped_img) < 35 or np.mean(cropped_img) > 220:\n",
        "                continue\n",
        "\n",
        "            if cropped_img.size[0] < 10 or cropped_img.size[1] < 10 :\n",
        "                continue\n",
        "\n",
        "            filename = f\"{count:06d}.jpg\"\n",
        "            cropped_img.save(os.path.join(save_dir, filename))\n",
        "\n",
        "            new_img_path = os.path.join(save_dir, filename)\n",
        "\n",
        "            label = new_img_path + \"\\t\" + label\n",
        "            labels.append(label)\n",
        "            count += 1\n",
        "\n",
        "    print(f\"Created {count} images\")\n",
        "\n",
        "    with open(os.path.join(save_dir, \"labels.txt\"), \"w\") as f:\n",
        "        f.write(\"\\n\".join(labels))\n",
        "\n",
        "save_dir = \"datasets/ocr_dataset\"\n",
        "split_bounding_boxes(img_paths, img_labels, bboxes, save_dir)\n"
      ],
      "metadata": {
        "id": "bEBXzBTddk47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = save_dir\n",
        "\n",
        "img_paths = []\n",
        "labels = []\n",
        "\n",
        "with open(os.path.join(root_dir, \"labels.txt\"), \"r\") as f:\n",
        "    for label in f:\n",
        "        labels.append(label.strip().split(\"\\t\")[1])\n",
        "        img_paths.append(label.strip().split(\"\\t\")[0])\n",
        "\n",
        "print(f\"Total images: {len(img_paths)}\")\n",
        "print(f\"Total labels: {len(labels)}\")"
      ],
      "metadata": {
        "id": "FYCvdMD8lsc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build vocabulary\n",
        "letters = [char.split(\".\")[0].lower() for char in labels]\n",
        "letters = \"\".join(letters)\n",
        "letters = sorted(list(set(letters)))\n",
        "\n",
        "blank_char = \"-\"\n",
        "chars += blank_char\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Vocab: {chars}\")\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8Qysbtyhoeei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
        "idx_to_char = {index: char for char, index in char_to_idx.items()}"
      ],
      "metadata": {
        "id": "9Ke62-PRq4p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_label_len = max([len(label) for label in labels])\n",
        "\n",
        "def encode(label, char_to_idx, max_label_len):\n",
        "    encoded_labels = torch.tensor(\n",
        "        [char_to_idx[char] for char in label], dtype = torch.int32\n",
        "\n",
        "    )\n",
        "    label_len = len(encoded_labels)\n",
        "    lengths = torch.tensor(\n",
        "        label_len,\n",
        "        dtype = torch.int32\n",
        "    )\n",
        "    padded_labels = F.pad(\n",
        "        encoded_labels,\n",
        "        (0, max_label_len - label_len)\n",
        "        value = 0\n",
        "    )\n",
        "\n",
        "    return padded_labels, lengths\n"
      ],
      "metadata": {
        "id": "gm3kkroBrllM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(encoded_sequences, idx_to_char, blank_char = \"-\"):\n",
        "    decoded_sequences  = []\n",
        "\n",
        "    for seq in encoded_sequences:\n",
        "        decoded_label = []\n",
        "        prev_char = None\n",
        "\n",
        "        for token in seq:\n",
        "            if token != 0:\n",
        "                char = idx_to_char[token.item()]\n",
        "\n",
        "                if char != blank_char:\n",
        "                    if char != prev_char or prev_char == blank_char:\n",
        "                        decoded_label.append(char)\n",
        "\n",
        "                prev_char = char\n",
        "\n",
        "        decoded_sequences.append(\"\".join(decoded_label))\n",
        "\n",
        "    print(f\"From {encoded_sequences} to {decoded_sequences}\")\n",
        "\n",
        "    return decoded_sequences"
      ],
      "metadata": {
        "id": "5VefLKf1xoPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    \"train\" : transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((100, 420))\n",
        "            transforms.ColorJitter(\n",
        "                brightness = 0.5,\n",
        "                contrast = 0.5,\n",
        "                saturation = 0.5,\n",
        "\n",
        "            ),\n",
        "            transforms.Grayscale(\n",
        "                num_output_channels = 1\n",
        "            ),\n",
        "            transforms.GaussianBlur(3),\n",
        "            transforms.RandomAffine(\n",
        "                degree = 1,\n",
        "                shear = 1\n",
        "            ),\n",
        "            transforms.RandomPerspective(\n",
        "                distortion_scale = 0.3,\n",
        "                p = 0.5\n",
        "                interpolation= 3\n",
        "            ),\n",
        "            transforms.RandomRotation(\n",
        "                degrees = 2\n",
        "            ),\n",
        "            transforms.ToTensor()\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "\n",
        "        ]\n",
        "    ),\n",
        "    \"val\" : transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((100, 420)),\n",
        "            transforms.Grayscale(\n",
        "                num_output_channels = 1\n",
        "            ),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ]\n",
        "    ),\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "FsGbnF040E29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "val_size = 0.2\n",
        "test_size = 0.1\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    img_paths,\n",
        "    labels,\n",
        "    test_size = val_size,\n",
        "    random_state = seed,\n",
        "    shuffle = is_shuffle,\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size = test_size,\n",
        "    random_state = seed,\n",
        "    shuffle = is_shuffle,\n",
        ")"
      ],
      "metadata": {
        "id": "JV2SILqd7KvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STRDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            X,\n",
        "            y,\n",
        "            char_to_idx,\n",
        "            max_label_len,\n",
        "            transform = None\n",
        "\n",
        "    ):\n",
        "        self.transforms = transform\n",
        "        self.img_paths = X\n",
        "        self.labels = y\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.max_label_len = max_label_len\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.img_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        if self.label_encoder:\n",
        "            encoded_label, label_len = self.label_encoder(\n",
        "                label, self.char_to_idx, self.max_label_len\n",
        "            )\n",
        "\n",
        "        return img, encoded_label, label_len\n"
      ],
      "metadata": {
        "id": "khyvUbqa9t4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = STRDataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    char_to_idx,\n",
        "    max_label_len,\n",
        "    label_encoder= encode,\n",
        "    data_transforms['train']\n",
        ")\n",
        "\n",
        "val_dataset = STRDataset(\n",
        "    X_val,\n",
        "    y_val,\n",
        "    char_to_idx,\n",
        "    max_label_len,\n",
        "    label_encoder= encode,\n",
        "    data_transforms['val']\n",
        ")\n",
        "\n",
        "test_dataset = STRDataset(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    char_to_idx,\n",
        "    max_label_len,\n",
        "    label_encoder= encode,\n",
        "    data_transforms['val']\n",
        ")\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 64 * 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size= train_batch_size,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size= test_batch_size,\n",
        "    shuffle = False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size= test_batch_size,\n",
        "    shuffle = False\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "Z5CIXO4EpcI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(\n",
        "            self, vocab_size, hidden_size, n_layers, dropout = 0.2, unfreeze_layers = 3\n",
        "    ):\n",
        "        super(CRNN, self).__init__()\n",
        "        backbone = timm.create_model(\"resnet152\", in_chans = 1, pretrained = True)\n",
        "        modules = list(backbone.children())[:-2]\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "\n",
        "        for parameter in self.backbone[-unfreeze_layers:].parameters():\n",
        "            parameter.requires_grad = True\n",
        "\n",
        "        self.mapSeq = nn.Sequential(\n",
        "            nn.Linear(2048,512), nn.ReLU(), nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            512,\n",
        "            hidden_size,\n",
        "            n_layers,\n",
        "            bidirectional = True,\n",
        "            batch_first = True,\n",
        "            dropout = dropout if n_layers > 1 else 0\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, vocab_size),\n",
        "            nn.LogSoftmax(dim = 2)\n",
        "        )\n",
        "\n",
        "        @torch.autocast(device_type = \"cuda\")\n",
        "        def forward(self, x):\n",
        "            x = self.backbone(x)\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "            #Flatten the feature map\n",
        "            x = x.view(x.size(0), x.size(1), -1)\n",
        "            x = self.mapSeq(x)\n",
        "            x, _ = self.gru(x)\n",
        "            x = self.layer_norm(x)\n",
        "            x = self.out(x)\n",
        "            x = x.permute(1, 0, 2)\n",
        "\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "t8KbiTc0eoQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "n_layers = 3\n",
        "dropout_prob = 0.2\n",
        "unfreeze_layers = 3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CRNN(\n",
        "    vocab_size,\n",
        "    hidden_size,\n",
        "    n_layers,\n",
        "    dropout = dropout_prob,\n",
        "    unfreeze_layers = unfreeze_layers\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "rIPC9tCeoFyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, labels_len in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels_len = labels_len.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            logits_lens = torch.full(\n",
        "                size = (outputs.size(1),),\n",
        "                fill_value = outputs.size(0),\n",
        "                dtype = torch.long\n",
        "            ).to(device)\n",
        "\n",
        "            loss = criterion(outputs, labels, logits_lens, labels_len)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "def fit(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs\n",
        "):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        batch_train_losses = []\n",
        "        for idx, (inputs, labels, labels_len) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels_len = labels_len.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            logits_lens = torch.full(\n",
        "                size = (outputs.size(1),),\n",
        "                fill_value = outputs.size(0),\n",
        "                dtype = torch.long\n",
        "            ).to(device)\n",
        "\n",
        "            loss = criterion(outputs, labels, logits_lens, labels_len)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(batch_train_losses)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(\n",
        "            f\"EPOCH {epoch + 1}:\\tTrain loss: {train_loss: .4f}\\tVal loss : {val_loss: .4f}\\t\\t Time: {time.time() - start:.2f} seconds\"\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "cTN_VhrCucll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lr = 5e-4\n",
        "weight_decay = 1e-5\n",
        "scheduler_step_size = epochs * 0.5\n",
        "\n",
        "criterion = nn.CTCLoss(\n",
        "    blank = char_to_idx[blank_char]\n",
        "\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "scheduler = torch.optim.lr_schduler.StepLR(optimizer, step_size = scheduler_step_size, gamma = 0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "m0ctuok74h46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "    device, epochs\n",
        ")"
      ],
      "metadata": {
        "id": "Q5iN8_bYWfaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "kq1zm1UkWv5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = evaluate(model, val_loader, criterion, device)\n",
        "test_loss = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"Evaluation on val/test dataset\")\n",
        "print(f\"Val loss: {val_loss:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "pnEjv04bWxeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_path = \"ocr_crnn.pt\"\n",
        "torch.save(model.state_dict(), save_model_path)"
      ],
      "metadata": {
        "id": "MVSQX0S4YOIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "xy-KO5tiYZEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_det_model_path = 'run/detect/train/weights/best.pt'\n",
        "yolo = YOLO(text_det_model_path)"
      ],
      "metadata": {
        "id": "9HfVEGJaZuNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text recognition"
      ],
      "metadata": {
        "id": "4R62C6LqbJHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers, dropout = 0.2, unfreeze_layers = 3):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        backbone = timm.create_model(\"resnet101\", in_chans = 1, pretrained = True)\n",
        "        modules = list(backbone.children())[:-2]\n",
        "        modules.append(nn.AdaptiveAvgPool2d(1, None))\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "\n",
        "        for parameter in self.backbone[-unfreeze_layers:].parameters():\n",
        "            parameter.requires_grad = True\n",
        "\n",
        "        self.mapSeq = nn.Sequential(\n",
        "            nn.Linear(2048,512), nn.ReLU(), nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            512,\n",
        "            hidden_size,\n",
        "            n_layers,\n",
        "            bidirectional = True,\n",
        "            batch_first = True,\n",
        "            dropout = dropout if n_layers > 1 else 0\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, vocab_size),\n",
        "            nn.LogSoftmax(dim = 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.permute(0, 3,1,2)\n",
        "        x.view(x.size(0), x.size(1), -1)\n",
        "        x = self.mapSeq(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.out(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "c0OgDNkQbHWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = '0123456789abcdefghijklmnopqrstuvwxyz-'\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "hidden_size = 256\n",
        "n_layers = 3\n",
        "dropout_prob = 0.2\n",
        "unfreeze_layers = 3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_path = 'ocr_crnn_base_best.pt'\n",
        "\n",
        "crnn_model = CRNN(\n",
        "    vocab_size,\n",
        "    hidden_size,\n",
        "    n_layers,\n",
        "    dropout = dropout_prob,\n",
        "    unfreeze_layers = unfreeze_layers\n",
        ").to(device)\n",
        "\n",
        "crnn_model.load_state_dict(torch.load(model_path))\n"
      ],
      "metadata": {
        "id": "OzJbk6zNYyOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(encoded_sequences, idx_to_char, blank_char = '-'):\n",
        "    decoded_sequences = []\n",
        "\n",
        "    for seq in encoded_sequences:\n",
        "        decoded_label = []\n",
        "        for idx, token in enumerate(seq):\n",
        "            if token != 0:\n",
        "                char = idx_to_char[token.item()]\n",
        "                if char != blank_char:\n",
        "                    decoded_label.append\n",
        "\n",
        "        decoded_sequences.append(''.join(decoded_label))\n",
        "\n",
        "    return decoded_sequences"
      ],
      "metadata": {
        "id": "Xc4C_OuhZOst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_detection(img_path, text_det_model):\n",
        "    text_det_results =  text_det_model(img_path, verbose = False)[0]\n",
        "\n",
        "    bboxes = text_det_results.boxes.xyxy.tolist()\n",
        "    classes = text_det_results.boxes.cls.tolist()\n",
        "    names = text_det_results.names\n",
        "    confs = text_det_results.boxes.conf.tolist()\n",
        "\n",
        "    return bboxes, classes, names, confs\n"
      ],
      "metadata": {
        "id": "q_56SJuboVgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_recognition(img, data_transforms, text_reg_model, idx_to_char, device):\n",
        "    transformed_img = data_transforms(img)\n",
        "    transformed_img = transformed_img.unsqueeze(0)\n",
        "    transformed_img = transformed_img.to(device)\n",
        "    text_reg_model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = text_reg_model(transformed_img).detach().cpu()\n",
        "    text = decode(logits.permute(1, 0, 2).argmax(2), idx_to_char)\n",
        "\n",
        "    return text\n",
        ""
      ],
      "metadata": {
        "id": "gnNor7qYtnyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_detections(img, detections):\n",
        "    plt.figure(figsize = (12,8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    for bbox, detected_class, confidence, transcribed_text in detections:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill = False, edgecolor = 'red', linewidth = 2))\n",
        "        plt.text(\n",
        "            x1, y1 - 10, f\"{detected_class}: {confidence:.2f}\\n{transcribed_text}\",\n",
        "            fontsize = 9, bbox = dict(facecolor = 'red', alpha = 0.5)\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "H7XzK-yGmjFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train' : transforms.Compose([\n",
        "        transforms.Resize((100, 420)),\n",
        "        transforms.ColorJitter(brightness = 0.5, contrast = 0.5, saturation = 0.5),\n",
        "        transforms.Grayscale(num_output_channels = 1),\n",
        "        transforms.GaussianBlur(3),\n",
        "        transforms.RandomPerpective(distortion_scale = 0.2, p = 0.3,interpolation = 3),\n",
        "        transforms.RandomRotation(degree = 2)\n",
        "        transforms.ToTensor()\n",
        "        transforms.Normalize(mean = [0.5], std = [0.5])\n",
        "    ]),\n",
        "    'val' : transforms.Compose([\n",
        "        transforms.Resize((100, 420)),\n",
        "        transforms.Grayscale(num_output_channels = 1),\n",
        "        transforms.ToTensor()\n",
        "        transforms.Normalize(mean = [0.5], std = [0.5])\n",
        "\n",
        "    ])\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "t_coZT0LqQDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(img_path, data_transforms, text_det_model, text_reg_model,\n",
        "            idx_to_char, device):\n",
        "    bboxes, classes, names, confs = text_detection(img_path, text_det_model)\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    predictions = []\n",
        "    for bbox, cls, conf in zip(bboxes, classes, confs):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        confidence = conf\n",
        "        detected_class = cls\n",
        "        name = names[int(cls)]\n",
        "\n",
        "        cropped_image = img.crop((x1, y1, x2, y2))\n",
        "        text = text_recognition(cropped_image, data_transforms, text_reg_model, idx_to_char, device)\n",
        "\n",
        "        predictions.append((bbox, name, confidence, text))\n",
        "\n",
        "    visualize_detections(img, predictions)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "QK7RD0TjqPCM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}